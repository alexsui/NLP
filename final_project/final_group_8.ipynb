{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/.conda/envs/hw2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/samuel/.conda/envs/hw2/lib/python3.9/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging,Trainer\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False # silence the warnings\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token=True\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_text(data):\n",
    "    prompt = '''[INST]Below is an English context with  user's question, situational background information, and my response. Your task is to judge whether my response to the user's question is appropriate or not only according to the situational background information provided. The user's question, situational background information and my response are specified with [User's question], [Background] and [My response] in the context respectively. If you think my response is appropriate, please answer \"Yes\"; if you think my response is inappropriate, please answer \"No\".'''\n",
    "    question = data['u']\n",
    "    response = data['r']\n",
    "    backgorund_info = f\"[Background]:\"\n",
    "    for i,(type, bg) in enumerate(zip(data['s.type'],data['s'])):\n",
    "        # backgorund_info += f\"\\n {i+1}. [{type}] {bg} [/{type}]\"\n",
    "        backgorund_info +=  f\"\\n {i+1}. {bg}\"\n",
    "    backgorund_info +=f\"\\n[/Background]\"\n",
    "    # backgorund_info = f\"[Background]: {background} [/Background]\"\n",
    "    text= f\"{prompt} \\n\\n[User's question]: {question} [/User's question] \\n{backgorund_info}\\n[My response]: {response} [/My response][/INST]\"\n",
    "    return text\n",
    "def formatting_label(data):\n",
    "    label = \"yes\"if data['r.label'] ==1 else \"no\"\n",
    "    relevant_background = [d+1 for d in sorted(data['s.gold.index'])]\n",
    "    if label==\"yes\":\n",
    "        label_text = f\"First, I identify the background statements relevant to user's question by number: {relevant_background}.\\nBased on the relevant background statements, the answer is <answer>{label.capitalize()}, your response is appropriate.</answer>\"\n",
    "    else:\n",
    "        label_text = f\"First, I identify the background statements relevant to user's question by number: {relevant_background}.\\nBased on the relevant background statements, the answer is <answer>{label.capitalize()}, your response is inappropriate.</answer>\"\n",
    "    text = f\"{label_text}\"\n",
    "    return text\n",
    "def format_prompt(data):\n",
    "\n",
    "    prompt = f\"{data['text']}\\n{data['label']}\"\n",
    "    return prompt\n",
    "def read_json(mode):\n",
    "    with open(f\"dataset/{mode}.json\", 'r') as file:\n",
    "        data = json.load(file)\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "train_data = read_json(\"train\")\n",
    "valid_data = read_json(\"val\")\n",
    "test_data = read_json(\"test\")\n",
    "train_data['text'] = train_data.apply(formatting_text,axis=1)\n",
    "train_data['label'] = train_data.apply(formatting_label,axis=1)\n",
    "train_data['prompt'] = train_data.apply(format_prompt,axis=1)\n",
    "train_data = train_data[['text','label','prompt']]\n",
    "valid_data['text'] = valid_data.apply(formatting_text,axis=1)\n",
    "valid_data['label'] = valid_data.apply(formatting_label,axis=1)\n",
    "valid_data['prompt'] = valid_data.apply(format_prompt,axis=1)\n",
    "valid_data = valid_data[['text','label','prompt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]Below is an English context with  user's question, situational background information, and my response. Your task is to judge whether my response to the user's question is appropriate or not only according to the situational background information provided. The user's question, situational background information and my response are specified with [User's question], [Background] and [My response] in the context respectively. If you think my response is appropriate, please answer \"Yes\"; if you think my response is inappropriate, please answer \"No\". \n",
      "\n",
      "[User's question]: Can you turn on the TV for me, please? [/User's question] \n",
      "[Background]:\n",
      " 1. [user] has a letter to send.\n",
      " 2. There are some soft drinks in the refrigerator.\n",
      " 3. It is Sunday afternoon now.\n",
      " 4. [user] has two friends visiting.\n",
      " 5. [user] has protein powder in the kitchen.\n",
      " 6. [user] has a TV in the house.\n",
      " 7. [user] has milk in the fridge.\n",
      " 8. [user] is home.\n",
      " 9. The TV is currently off.\n",
      " 10. There are drinks and appetizers in the kitchen.\n",
      " 11. [user] is wearing their football jersey.\n",
      " 12. The apple looks soft and rotten.\n",
      "[/Background]\n",
      "[My response]: Sure. I will send a Zoom notification. [/My response][/INST]\n",
      "First, I identify the background statements relevant to user's question by number: [3, 4, 6, 8, 9, 10, 11].\n",
      "Based on the relevant background statements, the answer is <answer>No, your response is inappropriate.</answer>\n"
     ]
    }
   ],
   "source": [
    "print(train_data['prompt'][1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3696.000000\n",
       "mean     1416.364989\n",
       "std        47.081108\n",
       "min      1278.000000\n",
       "25%      1384.000000\n",
       "50%      1414.500000\n",
       "75%      1446.000000\n",
       "max      1622.000000\n",
       "Name: prompt, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['prompt'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3696/3696 [00:01<00:00, 2970.62 examples/s]\n",
      "Map: 100%|██████████| 792/792 [00:00<00:00, 3110.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(data): #full prompt\n",
    "    model_inputs = tokenizer(data['prompt'], padding=\"max_length\", max_length=1650,truncation=True, return_tensors='pt')\n",
    "    # label = tokenizer(data['label'], padding=\"max_length\", max_length=50,truncation=True, return_tensors='pt')\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
    "    return model_inputs\n",
    "new_train_data = datasets.Dataset.from_pandas(train_data,split='train')\n",
    "new_valid_data = datasets.Dataset.from_pandas(valid_data,split='train')\n",
    "tokenized_train_data = new_train_data.map(preprocess,remove_columns = new_train_data.column_names,batched=True)\n",
    "tokenized_valid_data = new_valid_data.map(preprocess,remove_columns = new_valid_data.column_names,batched=True)\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[ \"q_proj\",\n",
    "        \"k_proj\",\"\"\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./models/chat_model_finetune\", #baseline_with_complete_prompt\n",
    "    logging_dir=\"./logs/chat_model_finetune\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=100,\n",
    "    save_total_limit = 4,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    do_eval=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    metric_for_best_model = \"eval_loss\",    \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset = tokenized_valid_data,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=new_train_data,\n",
    "#     eval_dataset = new_valid_data,\n",
    "#     peft_config=peft_config,\n",
    "#     max_seq_length= 1150,\n",
    "#     dataset_text_field=\"prompt\",\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_arguments,\n",
    "#     packing= False,\n",
    "#     compute_metrics = compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "base_model_id = \"mistralai/Mistral-7B-instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = PeftModel.from_pretrained(base_model, \"./models/chat_version_with_full_prompt7/checkpoint-1800\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "def formatting_text(data):\n",
    "    # prompt = '''Below is an English context with  user's question, situational background information, and my response. Your task is to judge whether my response to the user's question is appropriate or not only according to the situational background information provided. The user's question, situational background information and my response are specified with [User's question], [Background] and [My response] in the context respectively. If you think my response is appropriate, please answer \"Yes\"; if you think my response is inappropriate, please answer \"No\".'''\n",
    "    prompt = '''[INST]Below is an English context with  user's question, situational background information, and my response. Your task is to judge whether my response to the user's question is appropriate or not only according to the situational background information provided. The user's question, situational background information and my response are specified with [User's question], [Background] and [My response] in the context respectively. If you think my response is appropriate, please answer \"Yes\"; if you think my response is inappropriate, please answer \"No\".'''\n",
    "    question = data['u']\n",
    "    response = data['r']\n",
    "    backgorund_info = f\"[Background]:\"\n",
    "    for i,(type, bg) in enumerate(zip(data['s.type'],data['s'])):\n",
    "        backgorund_info +=  f\"\\n {i+1}. {bg}\"\n",
    "    backgorund_info +=f\"\\n[/Background]\"\n",
    "    text= f\"{prompt} \\n\\n[User's question]: {question} [/User's question] \\n{backgorund_info}\\n[My response]: {response} [/My response][/INST]\"\n",
    "    return text\n",
    "def formatting_label(data):\n",
    "    label = \"yes\"if data['r.label'] ==1 else \"no\"\n",
    "    relevant_background = sorted(data['s.gold.index'])\n",
    "    if label==\"yes\":\n",
    "        label_text = f\"First, I identify the background statements relevant to user's question by number: {relevant_background}.\\nBased on the relevant background statements, the answer is <answer>{label.capitalize()}, your response is appropriate.</answer>\"\n",
    "    else:\n",
    "        label_text = f\"First, I identify the background statements relevant to user's question by number: {relevant_background}.\\nBased on the relevant background statements, the answer is <answer>{label.capitalize()}, your response is inappropriate.</answer>\"\n",
    "    text = f\"{label_text}\"\n",
    "    return text\n",
    "def format_prompt(data):\n",
    "\n",
    "    prompt = f\"{data['text']}\\n{data['label']}\"\n",
    "    return prompt\n",
    "def read_json(mode):\n",
    "    with open(f\"dataset/{mode}.json\", 'r') as file:\n",
    "        data = json.load(file)\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "test_data = read_json(\"test\")\n",
    "test_data['text'] = test_data.apply(formatting_text,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import StoppingCriteriaList,StoppingCriteria\n",
    "model.eval()\n",
    "predictions = []\n",
    "class StopWords(StoppingCriteria):\n",
    "    def __init__(self, tk, stop_words: list[str]):\n",
    "        self.tk = tk\n",
    "        self.stop_tokens = stop_words\n",
    "\n",
    "    def __call__(self, input_ids, *_) -> bool:\n",
    "        s = self.tk.batch_decode(input_ids)[0]\n",
    "        for t in self.stop_tokens:\n",
    "            if s.endswith(t):\n",
    "                return True\n",
    "        return False\n",
    "sw = StopWords(tokenizer, [\"</answer>\"])\n",
    "scl = StoppingCriteriaList([sw])\n",
    "with torch.no_grad():\n",
    "    for i in trange(len(test_data)):\n",
    "        eval_prompt = test_data['text'][i]\n",
    "        model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        pred = tokenizer.decode(model.generate(**model_input, max_new_tokens=500,pad_token_id = tokenizer.eos_token_id,stopping_criteria=scl)[0], skip_special_tokens=True)\n",
    "        pred = pred.replace(eval_prompt,\"\")\n",
    "        predictions.append(pred)\n",
    "        print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "for pre in predictions:\n",
    "    if \"Yes\" in pre or \"yes\" in pre:\n",
    "        ans.append(1)\n",
    "    elif \"No\" in pre or \"no\" in pre:\n",
    "        ans.append(0)\n",
    "    else:\n",
    "        ans.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247, 545)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([a for a in ans if a==1]),len([a for a in ans if a==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "ans= []\n",
    "predictions = pd.read_csv(\"predictions.csv\")['result']\n",
    "for pred in predictions:\n",
    "    text = pred\n",
    "    # Updated regex pattern to extract \"yes\" or \"no\" after \"### Answer:\"\n",
    "    pattern_flexible = r'<answer>(.*?)</answer>'\n",
    "\n",
    "\n",
    "    # Extracting the answer using the updated pattern\n",
    "    match_updated = re.search(pattern_flexible, text, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Extracting the answer\n",
    "    answer_updated = match_updated.group(1) if match_updated else \"No match found\"\n",
    "    ans.append(answer_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ans = [1 if a=='Yes' else 0 for a in ans]\n",
    "sum(final_ans)\n",
    "pd.DataFrame({\"response_quality\":final_ans}).to_csv(\"submission.csv\",index_label=\"index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
