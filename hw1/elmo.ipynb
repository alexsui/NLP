{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 11:14:54.540489: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-05 11:14:54.721758: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-05 11:14:54.721816: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-05 11:14:54.722294: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-05 11:14:54.806093: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-05 11:14:55.396698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_hub as hub\n",
    "# import tensorflow as tf\n",
    "# #Elmo\n",
    "# elmo = hub.load(\"https://tfhub.dev/google/elmo/3\").signatures[\"default\"]\n",
    "# input_tensor = np.array([\"my birthday is the best day \",\"today is a good day\",\"sdf \"])\n",
    "# embeddings_tensor = elmo(tf.constant(input_tensor))[\"elmo\"]  #, signature=\"default\", as_dict=True)\n",
    "# print(embeddings_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "/home/samuel/.conda/envs/tf/lib/python3.9/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/.conda/envs/tf/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "# import torch\n",
    "import nltk\n",
    "import random\n",
    "import re                                 \n",
    "import string\n",
    "from cleantext import clean            \n",
    "from nltk.corpus import stopwords     \n",
    "from nltk.stem import PorterStemmer       \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "    #     'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    segmenter=\"twitter\", \n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for  \n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "class Dataset(object):\n",
    "    def __init__(self, root_dir, max_seq_length, mode = \"train\", transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.text_processor = text_processor\n",
    "        self.data = self.read_data(root_dir)\n",
    "        if self.mode != \"test\":\n",
    "            self.labels = self.data.labels.tolist()\n",
    "        self.classes = list(pd.read_csv(\"./HW1_dataset/sample_submission.csv\").columns[1:])\n",
    "        self.tokenizer =  TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "        self.tweet_length,self.tweet_token = self.preprocess(self.data.tweet)\n",
    "    def preprocess(self, tweets):\n",
    "        def filter_words(x):\n",
    "            x = re.sub(r'@[A-Za-z0-9_]+', '', x)\n",
    "            # x = re.sub(r'[@#][^\\s]*\\s*|\\s*[@#][^\\s]*$', '', x)\n",
    "            # x = re.sub(r'RT[\\s]+', '', x)\n",
    "            # x = re.sub(r\"@([A-Za-z0-9_]{4,15})\", r\"@ <user>\", x)\n",
    "            # x = re.sub(r'\\$', '', x)\n",
    "            # x = re.sub(r'\\+', '', x)\n",
    "            # x = re.sub(r'\\|', '', x)\n",
    "            # x = re.sub(r'\\.\\.\\.', '', x)\n",
    "            x = x.replace(\"cant\",\"can't\")\n",
    "            x = x.replace(\"wont\",\"won't\")\n",
    "            x = x.replace(\"Im\",\"I'm\")\n",
    "            x = x.replace(\"Ill\",\"I'll\")\n",
    "            x = x.replace(\"Ive\",\"I have\")\n",
    "            x = x.replace(\"I've\",\"I have\")\n",
    "            x = x.replace(\"youre\",\"you're\")\n",
    "            x = x.replace(\"theyre\",\"they're\")\n",
    "            x = x.replace(\"thats\",\"that's\")\n",
    "            x = x.replace(\"whats\",\"what's\")\n",
    "            x = x.replace(\"dont\",\"don't\")\n",
    "            x = x.replace(\"didnt\",\"didn't\")\n",
    "            x = x.replace(\"doesnt\",\"doesn't\")\n",
    "            x = x.replace(\".\",\"\") \n",
    "            x = x.replace(\"nvm\",\"never mind\")\n",
    "            x = clean(x, no_emoji=True, no_urls =True, no_numbers=True, replace_with_url=\"\", replace_with_number=\"<NUMBER>\")\n",
    "            \n",
    "            return x\n",
    "        tweets = tweets.apply(lambda x : filter_words(x))\n",
    "        tweet_token = tweets.apply(lambda x : self.text_processor.pre_process_doc(x))\n",
    "        tweet_length = tweet_token.apply(lambda x : len(x))\n",
    "        return tweet_length.to_numpy(), tweet_token.to_numpy()\n",
    "    def padding(self,x):\n",
    "        tweets = []\n",
    "        for tweet in x:\n",
    "            if len(tweet)>=self.max_seq_length:\n",
    "                tweets.append(tweet[:self.max_seq_length])\n",
    "            else:\n",
    "                tweets.append(tweet + [\"\"]*(self.max_seq_length-len(tweet)))\n",
    "        return tweets\n",
    "    def encode_label(self, labels):\n",
    "        targets = np.zeros((len(labels),12))\n",
    "        \n",
    "        for i,label in enumerate(labels):\n",
    "            for l in label:\n",
    "                idx = self.classes.index(l)\n",
    "                targets[i,idx] = 1\n",
    "        return targets\n",
    "    def read_data(self, root_dir):\n",
    "        with open(root_dir, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        df =  pd.DataFrame(data)\n",
    "        if self.mode != \"test\":\n",
    "            df[\"labels\"] = df.labels.apply(lambda x : list(x.keys()))\n",
    "        return df \n",
    "    def get_data(self):\n",
    "        self.tweet_token = self.padding(self.tweet_token)\n",
    "        if self.mode != \"test\":\n",
    "            return [self.tweet_token,self.encode_label(self.labels)]\n",
    "        else:\n",
    "            return [self.tweet_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./HW1_dataset/train.json\"\n",
    "val_data_path = \"./HW1_dataset/val.json\"\n",
    "test_data_path = \"./HW1_dataset/test.json\"\n",
    "train_dataset = Dataset(train_data_path, 55, mode = \"train\", transform=None)\n",
    "val_dataset = Dataset(val_data_path, 55,mode = \"val\", transform=None)\n",
    "test_dataset = Dataset(test_data_path, 55,mode = \"test\", transform=None)\n",
    "train_data = train_dataset.get_data()\n",
    "val_data = val_dataset.get_data()\n",
    "test_data = test_dataset.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 11:15:36.632206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-05 11:15:36.657537: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\").signatures[\"default\"]\n",
    "seq_length = 55\n",
    "embedding_dim = 1024\n",
    "def generate_embeddings(data):\n",
    "    embeddings = np.zeros((len(data[0]),seq_length,embedding_dim))\n",
    "    for i in tqdm(range(len(data[0]))):\n",
    "        try:\n",
    "            data_tweet = tf.constant(data[0][i])\n",
    "            embeddings[i] = tf.squeeze(elmo(data_tweet)[\"elmo\"])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"index:\",i)\n",
    "            print(data_tweet.shape)\n",
    "            print(data_tweet)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 5346/6956 [09:35<02:37, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not broadcast input array from shape (55,3,1024) into shape (55,1024)\n",
      "index: 5344\n",
      "(55,)\n",
      "tf.Tensor(\n",
      "[b'now' b'should' b'we' b'trust' b'pfizer' b'in' b'injecting' b'our'\n",
      " b'bodies' b'( ? )' b'but' b'here' b'is' b'the' b'nigerian' b'meningitis'\n",
      " b'scandal' b'pfizer' b'was' b'involved' b'in' b'a' b'<number>' b'year'\n",
      " b'battle' b'that' b'is' b'now' b'one' b'of' b'the' b'biggest' b'scandals'\n",
      " b'in' b'the' b'pharmaceutical' b'world' b'note' b'it' b'was' b'regarding'\n",
      " b'a' b'drug' b'not' b'vaccine' b'https' b'://' b'tco' b'/' b'brvlcuv7cn'\n",
      " b'' b'' b'' b'' b''], shape=(55,), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 5458/6956 [09:45<02:20, 10.63it/s]"
     ]
    }
   ],
   "source": [
    "train_embeddings = generate_embeddings(train_data)\n",
    "val_embeddings = generate_embeddings(val_data)\n",
    "test_embeddings = generate_embeddings(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[1][5344] = np.zeros(12)\n",
    "np.save(\"./elmo_embedding/new_preprocess_noAnotate_elmo_X_train.npy\",train_embeddings)\n",
    "np.save(\"./elmo_embedding/new_preprocess_noAnotate_elmo_X_val.npy\",val_embeddings)\n",
    "np.save(\"./elmo_embedding/new_preprocess_noAnotate_elmo_X_test.npy\",test_embeddings)\n",
    "np.save(\"./elmo_embedding/new_preprocess_noAnotate_elmo_y_train.npy\",train_data[1])\n",
    "np.save(\"./elmo_embedding/new_preprocess_noAnotate_elmo_y_val.npy\",val_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
